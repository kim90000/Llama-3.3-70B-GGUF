{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yg9h2lVbDHOZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/bartowski/Llama-3.3-70B-Instruct-GGUF/resolve/main/Llama-3.3-70B-Instruct-IQ1_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CCCsTILDnwK",
        "outputId": "5382277c-b4e0-4fe3-d247-2ec4ac8422dd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-09 20:13:55--  https://huggingface.co/bartowski/Llama-3.3-70B-Instruct-GGUF/resolve/main/Llama-3.3-70B-Instruct-IQ1_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.169.137.119, 3.169.137.5, 3.169.137.111, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.169.137.119|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/9a/3f/9a3fcd012ee3fb810b70d4805776d5c5b67846364c74dcffe761f0dcffeca8b5/4253febb74e420fde26c0c6bb250ca06ef2ae0a15dfc8fe669c773ff5674c74d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.3-70B-Instruct-IQ1_M.gguf%3B+filename%3D%22Llama-3.3-70B-Instruct-IQ1_M.gguf%22%3B&Expires=1734034436&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNDAzNDQzNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzlhLzNmLzlhM2ZjZDAxMmVlM2ZiODEwYjcwZDQ4MDU3NzZkNWM1YjY3ODQ2MzY0Yzc0ZGNmZmU3NjFmMGRjZmZlY2E4YjUvNDI1M2ZlYmI3NGU0MjBmZGUyNmMwYzZiYjI1MGNhMDZlZjJhZTBhMTVkZmM4ZmU2NjljNzczZmY1Njc0Yzc0ZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=ZDkmvimye1mGqDkrMcE8tnU4AeX%7EsJ-qOMjOeH54D86MHeS%7EE5BI3HYsLov6HoFFMX6v8mCwslLAWJNfmDmyw1s5dIuNDuDXmDhQDNEGOyP%7EQWUhq-3FMWTgW4Nr4gaw4JBMCtY4jE1sgYAx4j6H5Y5eUmO3KvHWoHXpN%7EB2%7ELTEGEJSbKjhHhFIxAzQ%7EKCfVMCyr--2RdVpNN4daAr8xG4oGmGE0pZcL814K-i8yg4AXR-6FbiX-p%7EJEpgDhuzrE13Mk46UN9%7Ei-vK8vP0GT7ApdbiUe035kZcwm2hH6m5t5sU1TDLcNbpbwsPV1jyGxDmduIg4teYNA2mKtFjB9A__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2024-12-09 20:13:56--  https://cdn-lfs-us-1.hf.co/repos/9a/3f/9a3fcd012ee3fb810b70d4805776d5c5b67846364c74dcffe761f0dcffeca8b5/4253febb74e420fde26c0c6bb250ca06ef2ae0a15dfc8fe669c773ff5674c74d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.3-70B-Instruct-IQ1_M.gguf%3B+filename%3D%22Llama-3.3-70B-Instruct-IQ1_M.gguf%22%3B&Expires=1734034436&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNDAzNDQzNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzlhLzNmLzlhM2ZjZDAxMmVlM2ZiODEwYjcwZDQ4MDU3NzZkNWM1YjY3ODQ2MzY0Yzc0ZGNmZmU3NjFmMGRjZmZlY2E4YjUvNDI1M2ZlYmI3NGU0MjBmZGUyNmMwYzZiYjI1MGNhMDZlZjJhZTBhMTVkZmM4ZmU2NjljNzczZmY1Njc0Yzc0ZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=ZDkmvimye1mGqDkrMcE8tnU4AeX%7EsJ-qOMjOeH54D86MHeS%7EE5BI3HYsLov6HoFFMX6v8mCwslLAWJNfmDmyw1s5dIuNDuDXmDhQDNEGOyP%7EQWUhq-3FMWTgW4Nr4gaw4JBMCtY4jE1sgYAx4j6H5Y5eUmO3KvHWoHXpN%7EB2%7ELTEGEJSbKjhHhFIxAzQ%7EKCfVMCyr--2RdVpNN4daAr8xG4oGmGE0pZcL814K-i8yg4AXR-6FbiX-p%7EJEpgDhuzrE13Mk46UN9%7Ei-vK8vP0GT7ApdbiUe035kZcwm2hH6m5t5sU1TDLcNbpbwsPV1jyGxDmduIg4teYNA2mKtFjB9A__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.169.36.31, 3.169.36.38, 3.169.36.120, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.169.36.31|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16751201248 (16G) [binary/octet-stream]\n",
            "Saving to: ‘Llama-3.3-70B-Instruct-IQ1_M.gguf’\n",
            "\n",
            "Llama-3.3-70B-Instr 100%[===================>]  15.60G  40.1MB/s    in 6m 39s  \n",
            "\n",
            "2024-12-09 20:20:35 (40.0 MB/s) - ‘Llama-3.3-70B-Instruct-IQ1_M.gguf’ saved [16751201248/16751201248]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ggerganov/llama.cpp/releases/download/b4273/llama-b4273-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAXtGDb5DqAy",
        "outputId": "a9d24a7e-cbda-4989-d093-586f952ba575"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-09 20:20:36--  https://github.com/ggerganov/llama.cpp/releases/download/b4273/llama-b4273-bin-ubuntu-x64.zip\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/5bf371c2-6a04-404d-aeb4-5ae58fdb9194?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241209%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241209T202037Z&X-Amz-Expires=300&X-Amz-Signature=4477173a638819b6fac67f4be4f2bac6f58223eb00aab51ec86486d78d439a08&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b4273-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-09 20:20:37--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/5bf371c2-6a04-404d-aeb4-5ae58fdb9194?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241209%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241209T202037Z&X-Amz-Expires=300&X-Amz-Signature=4477173a638819b6fac67f4be4f2bac6f58223eb00aab51ec86486d78d439a08&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b4273-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 61829095 (59M) [application/octet-stream]\n",
            "Saving to: ‘llama-b4273-bin-ubuntu-x64.zip’\n",
            "\n",
            "llama-b4273-bin-ubu 100%[===================>]  58.96M  22.0MB/s    in 2.7s    \n",
            "\n",
            "2024-12-09 20:20:41 (22.0 MB/s) - ‘llama-b4273-bin-ubuntu-x64.zip’ saved [61829095/61829095]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llama-b4273-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzZwbM7EFN-6",
        "outputId": "593b5424-6daf-4494-fed5-6e63c1c051ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama-b4273-bin-ubuntu-x64.zip\n",
            "  inflating: build/bin/LICENSE       \n",
            "  inflating: build/bin/llama-batched  \n",
            "  inflating: build/bin/llama-batched-bench  \n",
            "  inflating: build/bin/llama-bench   \n",
            "  inflating: build/bin/llama-cli     \n",
            "  inflating: build/bin/llama-convert-llama2c-to-ggml  \n",
            "  inflating: build/bin/llama-cvector-generator  \n",
            "  inflating: build/bin/llama-embedding  \n",
            "  inflating: build/bin/llama-eval-callback  \n",
            "  inflating: build/bin/llama-export-lora  \n",
            "  inflating: build/bin/llama-gbnf-validator  \n",
            "  inflating: build/bin/llama-gguf    \n",
            "  inflating: build/bin/llama-gguf-hash  \n",
            "  inflating: build/bin/llama-gguf-split  \n",
            "  inflating: build/bin/llama-gritlm  \n",
            "  inflating: build/bin/llama-imatrix  \n",
            "  inflating: build/bin/llama-infill  \n",
            "  inflating: build/bin/llama-llava-cli  \n",
            "  inflating: build/bin/llama-lookahead  \n",
            "  inflating: build/bin/llama-lookup  \n",
            "  inflating: build/bin/llama-lookup-create  \n",
            "  inflating: build/bin/llama-lookup-merge  \n",
            "  inflating: build/bin/llama-lookup-stats  \n",
            "  inflating: build/bin/llama-minicpmv-cli  \n",
            "  inflating: build/bin/llama-parallel  \n",
            "  inflating: build/bin/llama-passkey  \n",
            "  inflating: build/bin/llama-perplexity  \n",
            "  inflating: build/bin/llama-q8dot   \n",
            "  inflating: build/bin/llama-quantize  \n",
            "  inflating: build/bin/llama-quantize-stats  \n",
            "  inflating: build/bin/llama-retrieval  \n",
            "  inflating: build/bin/llama-run     \n",
            "  inflating: build/bin/llama-save-load-state  \n",
            "  inflating: build/bin/llama-server  \n",
            "  inflating: build/bin/llama-simple  \n",
            "  inflating: build/bin/llama-simple-chat  \n",
            "  inflating: build/bin/llama-speculative  \n",
            "  inflating: build/bin/llama-speculative-simple  \n",
            "  inflating: build/bin/llama-tokenize  \n",
            "  inflating: build/bin/llama-vdot    \n",
            "  inflating: build/bin/rpc-server    \n",
            "  inflating: build/bin/test-arg-parser  \n",
            "  inflating: build/bin/test-autorelease  \n",
            "  inflating: build/bin/test-backend-ops  \n",
            "  inflating: build/bin/test-barrier  \n",
            "  inflating: build/bin/test-c        \n",
            "  inflating: build/bin/test-chat-template  \n",
            "  inflating: build/bin/test-grammar-integration  \n",
            "  inflating: build/bin/test-grammar-parser  \n",
            "  inflating: build/bin/test-json-schema-to-grammar  \n",
            "  inflating: build/bin/test-llama-grammar  \n",
            "  inflating: build/bin/test-log      \n",
            "  inflating: build/bin/test-model-load-cancel  \n",
            "  inflating: build/bin/test-quantize-fns  \n",
            "  inflating: build/bin/test-quantize-perf  \n",
            "  inflating: build/bin/test-rope     \n",
            "  inflating: build/bin/test-sampling  \n",
            "  inflating: build/bin/test-tokenizer-0  \n",
            "  inflating: build/bin/test-tokenizer-1-bpe  \n",
            "  inflating: build/bin/test-tokenizer-1-spm  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/llama-cli -m PATH_TO_MODEL -p \"Building a website can be done in 10 steps:\" -ngl 32"
      ],
      "metadata": {
        "id": "JmYkoRzWDzTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/Llama-3.3-70B-Instruct-IQ1_M.gguf"
      ],
      "metadata": {
        "id": "aP9hlZpfFXjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/llama-cli -m /content/Llama-3.3-70B-Instruct-IQ1_M.gguf -p \"Building a website can be done in 10 steps:\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suxAjkx4FZUo",
        "outputId": "e79e857e-b3a9-42d3-e395-5206daf7feb0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 4273 (c9c6e01d) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 40 key-value pairs and 724 tensors from /content/Llama-3.3-70B-Instruct-IQ1_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.3 70B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.3\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 70B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.3\n",
            "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Llama 3.1 70B\n",
            "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Meta Llama\n",
            "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...\n",
            "llama_model_loader: - kv  11:                               general.tags arr[str,5]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv  12:                          general.languages arr[str,8]       = [\"en\", \"fr\", \"it\", \"pt\", \"hi\", \"es\", ...\n",
            "llama_model_loader: - kv  13:                          llama.block_count u32              = 80\n",
            "llama_model_loader: - kv  14:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  15:                     llama.embedding_length u32              = 8192\n",
            "llama_model_loader: - kv  16:                  llama.feed_forward_length u32              = 28672\n",
            "llama_model_loader: - kv  17:                 llama.attention.head_count u32              = 64\n",
            "llama_model_loader: - kv  18:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  19:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  20:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  21:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  22:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  23:                          general.file_type u32              = 31\n",
            "llama_model_loader: - kv  24:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  25:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  36:                      quantize.imatrix.file str              = /models_out/Llama-3.3-70B-Instruct-GG...\n",
            "llama_model_loader: - kv  37:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  38:             quantize.imatrix.entries_count i32              = 560\n",
            "llama_model_loader: - kv  39:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:  162 tensors\n",
            "llama_model_loader: - type q2_K:   11 tensors\n",
            "llama_model_loader: - type q4_K:   80 tensors\n",
            "llama_model_loader: - type q5_K:    1 tensors\n",
            "llama_model_loader: - type iq2_xxs:   80 tensors\n",
            "llama_model_loader: - type iq1_m:  390 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 8192\n",
            "llm_load_print_meta: n_layer          = 80\n",
            "llm_load_print_meta: n_head           = 64\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 28672\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 70B\n",
            "llm_load_print_meta: model ftype      = IQ1_M - 1.75 bpw\n",
            "llm_load_print_meta: model params     = 70.55 B\n",
            "llm_load_print_meta: model size       = 15.59 GiB (1.90 BPW) \n",
            "llm_load_print_meta: general.name     = Llama 3.3 70B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128004 '<|finetune_right_pad_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 0 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 0/81 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size = 15967.69 MiB\n",
            "................................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 4096\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1280.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   584.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 2566\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "\n",
            "sampler seed: 662716099\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\n",
            "\n",
            "Building a website can be done in 10 steps: \n",
            "\n",
            "1\n",
            "llama_perf_sampler_print:    sampling time =       0.37 ms /    14 runs   (    0.03 ms per token, 38043.48 tokens per second)\n",
            "llama_perf_context_print:        load time =  175997.98 ms\n",
            "llama_perf_context_print: prompt eval time =  255685.55 ms /    12 tokens (21307.13 ms per token,     0.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =   65911.65 ms /     1 runs   (65911.65 ms per token,     0.02 tokens per second)\n",
            "llama_perf_context_print:       total time =  382722.75 ms /    13 tokens\n",
            "Interrupted by user\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio spaces"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJNNXxNIHx2g",
        "outputId": "74941d85-73bf-40b3-da8b-e9f94ba92f10"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.8.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting spaces\n",
            "  Downloading spaces-0.30.4-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.5.1 (from gradio)\n",
            "  Downloading gradio_client-1.5.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.1->gradio) (2024.10.0)\n",
            "Collecting websockets<15.0,>=10.0 (from gradio-client==1.5.1->gradio)\n",
            "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: psutil<6,>=2 in /usr/local/lib/python3.10/dist-packages (from spaces) (5.9.5)\n",
            "Requirement already satisfied: requests<3.0,>=2.19 in /usr/local/lib/python3.10/dist-packages (from spaces) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.19->spaces) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.19->spaces) (2.2.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.8.0-py3-none-any.whl (57.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.2/320.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spaces-0.30.4-py3-none-any.whl (27 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.8.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio, spaces\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.4.0 gradio-5.8.0 gradio-client-1.5.1 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.19 ruff-0.8.2 safehttpx-0.1.6 semantic-version-2.10.0 spaces-0.30.4 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.32.1 websockets-14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ROlln84oHoQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "# مسار نموذج GGUF\n",
        "model_path = \"/content/Llama-3.3-70B-Instruct-IQ1_M.gguf\"\n",
        "\n",
        "# وظيفة مخصصة لتشغيل نموذج GGUF\n",
        "def run_llama_cpp(prompt):\n",
        "  # تشغيل llama.cpp باستخدام os.system\n",
        "  command = f\"./build/bin/llama-cli -m {model_path} -p \\\"{prompt}\\\"\"\n",
        "  os.system(command)\n",
        "  # قراءة الإخراج من الملف الناتج\n",
        "  with open(\"output.txt\", \"r\") as f:\n",
        "    output = f.read()\n",
        "  return output\n",
        "\n",
        "# إنشاء واجهة Gradio\n",
        "iface = gr.Interface(\n",
        "    fn=run_llama_cpp,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your prompt here...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Llama-3.3-70B-Instruct-GGUF\",\n",
        ")\n",
        "\n",
        "# تشغيل الواجهة\n",
        "iface.launch(share=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "XyolT1upIFDw",
        "outputId": "73d6aaf3-1858-44e0-d57b-8776412a8f5b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e6011b71c1e89bf78e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e6011b71c1e89bf78e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}